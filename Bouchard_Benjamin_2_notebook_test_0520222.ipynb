{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from html.parser import HTMLParser\n",
    "from itertools import accumulate,chain,takewhile\n",
    "import itertools\n",
    "import warnings\n",
    "import itertools\n",
    "from pathlib import Path,PureWindowsPath,PurePosixPath\n",
    "\n",
    "#wordcloud\n",
    "import wordcloud\n",
    "\n",
    "# \n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#pygments\n",
    "from pygments.lexers import guess_lexer\n",
    "from pygments.util  import ClassNotFound\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import jaccard_score,confusion_matrix\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "\n",
    "#bitarray\n",
    "# from  bitarray import bitarray\n",
    "# from  bitarray.util import ba2int\n",
    "\n",
    "#joblib\n",
    "import joblib\n",
    "\n",
    "#local\n",
    "from Bouchard_Benjamin_1_notebook_exploration_0520222 import *\n",
    "import vectorizer as vectz\n",
    "import lda as ldam\n",
    "\n",
    "# from importlib import reload\n",
    "# reload(vectz)\n",
    "warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_list(df,cols):\n",
    "    to_list_ = lambda x: list(map(lambda y: y.replace(\"'\",\"\"),x.replace(\"[\",\"\").replace(\"]\",\"\").split(\", \")))\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(to_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "      <th>ntags</th>\n",
       "      <th>contents</th>\n",
       "      <th>body-tokens</th>\n",
       "      <th>body-tokens-wov</th>\n",
       "      <th>title-tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SQL Server 2008 Full Text Search (FTS) versus ...</td>\n",
       "      <td>&lt;p&gt;I know there have been questions in the pas...</td>\n",
       "      <td>&lt;sql-server&gt;&lt;sql-server-2008&gt;&lt;full-text-search...</td>\n",
       "      <td>499247</td>\n",
       "      <td>40</td>\n",
       "      <td>18582</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>[sql-server, sql-server-2008, full-text-search...</td>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'p': ['I know the...</td>\n",
       "      <td>[question, sql, change, wonder, pro, con, link...</td>\n",
       "      <td>[change, pro, con, article]</td>\n",
       "      <td>[sql, server, search, ft, versus, net]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>XML Serialization and Inherited Types</td>\n",
       "      <td>&lt;p&gt;Following on from my &lt;a href=\"https://stack...</td>\n",
       "      <td>&lt;c#&gt;&lt;xml&gt;&lt;inheritance&gt;&lt;serialization&gt;&lt;xml-seri...</td>\n",
       "      <td>20084</td>\n",
       "      <td>86</td>\n",
       "      <td>56816</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>[c#, xml, inheritance, serialization, xml-seri...</td>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'p': ['Following ...</td>\n",
       "      <td>[problem, collection, base, class, type, popul...</td>\n",
       "      <td>[collection, base, class, type, type, add, xml...</td>\n",
       "      <td>[serialization, inherit, type]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MyISAM versus InnoDB</td>\n",
       "      <td>&lt;p&gt;I'm working on a projects which involves a ...</td>\n",
       "      <td>&lt;mysql&gt;&lt;database&gt;&lt;performance&gt;&lt;innodb&gt;&lt;myisam&gt;</td>\n",
       "      <td>20148</td>\n",
       "      <td>887</td>\n",
       "      <td>301985</td>\n",
       "      <td>390</td>\n",
       "      <td>25</td>\n",
       "      <td>[mysql, database, performance, innodb, myisam]</td>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'p': [\"I'm workin...</td>\n",
       "      <td>[work, project, involve, database, write, bunc...</td>\n",
       "      <td>[project, database, bunch, stuff, web, differe...</td>\n",
       "      <td>[myisam, versus, innodb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Recommended SQL database design for tags or ta...</td>\n",
       "      <td>&lt;p&gt;I've heard of a few ways to implement taggi...</td>\n",
       "      <td>&lt;sql&gt;&lt;database-design&gt;&lt;tags&gt;&lt;data-modeling&gt;&lt;ta...</td>\n",
       "      <td>20856</td>\n",
       "      <td>325</td>\n",
       "      <td>118552</td>\n",
       "      <td>307</td>\n",
       "      <td>6</td>\n",
       "      <td>[sql, database-design, tags, data-modeling, ta...</td>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'p': [\"I've heard...</td>\n",
       "      <td>[heard, way, implement, tag, mapping, itemid, ...</td>\n",
       "      <td>[heard, way, mapping, itemid, sense, scale, nu...</td>\n",
       "      <td>[recommend, database, design, tag, tag]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Specifying a mySQL ENUM in a Django model</td>\n",
       "      <td>&lt;p&gt;How do I go about specifying and using an E...</td>\n",
       "      <td>&lt;python&gt;&lt;mysql&gt;&lt;django&gt;&lt;django-models&gt;&lt;enums&gt;</td>\n",
       "      <td>21454</td>\n",
       "      <td>99</td>\n",
       "      <td>61572</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>[python, mysql, django, django-models, enums]</td>\n",
       "      <td>defaultdict(&lt;class 'list'&gt;, {'p': ['How do I g...</td>\n",
       "      <td>[enum, django, model]</td>\n",
       "      <td>[enum, django, model]</td>\n",
       "      <td>[mysql, enum, django, model]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           0  SQL Server 2008 Full Text Search (FTS) versus ...   \n",
       "1           1              XML Serialization and Inherited Types   \n",
       "2           2                               MyISAM versus InnoDB   \n",
       "3           3  Recommended SQL database design for tags or ta...   \n",
       "4           4          Specifying a mySQL ENUM in a Django model   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>I know there have been questions in the pas...   \n",
       "1  <p>Following on from my <a href=\"https://stack...   \n",
       "2  <p>I'm working on a projects which involves a ...   \n",
       "3  <p>I've heard of a few ways to implement taggi...   \n",
       "4  <p>How do I go about specifying and using an E...   \n",
       "\n",
       "                                                Tags      Id  Score  \\\n",
       "0  <sql-server><sql-server-2008><full-text-search...  499247     40   \n",
       "1  <c#><xml><inheritance><serialization><xml-seri...   20084     86   \n",
       "2     <mysql><database><performance><innodb><myisam>   20148    887   \n",
       "3  <sql><database-design><tags><data-modeling><ta...   20856    325   \n",
       "4      <python><mysql><django><django-models><enums>   21454     99   \n",
       "\n",
       "   ViewCount  FavoriteCount  AnswerCount  \\\n",
       "0      18582             26            5   \n",
       "1      56816             42            7   \n",
       "2     301985            390           25   \n",
       "3     118552            307            6   \n",
       "4      61572             21            9   \n",
       "\n",
       "                                               ntags  \\\n",
       "0  [sql-server, sql-server-2008, full-text-search...   \n",
       "1  [c#, xml, inheritance, serialization, xml-seri...   \n",
       "2     [mysql, database, performance, innodb, myisam]   \n",
       "3  [sql, database-design, tags, data-modeling, ta...   \n",
       "4      [python, mysql, django, django-models, enums]   \n",
       "\n",
       "                                            contents  \\\n",
       "0  defaultdict(<class 'list'>, {'p': ['I know the...   \n",
       "1  defaultdict(<class 'list'>, {'p': ['Following ...   \n",
       "2  defaultdict(<class 'list'>, {'p': [\"I'm workin...   \n",
       "3  defaultdict(<class 'list'>, {'p': [\"I've heard...   \n",
       "4  defaultdict(<class 'list'>, {'p': ['How do I g...   \n",
       "\n",
       "                                         body-tokens  \\\n",
       "0  [question, sql, change, wonder, pro, con, link...   \n",
       "1  [problem, collection, base, class, type, popul...   \n",
       "2  [work, project, involve, database, write, bunc...   \n",
       "3  [heard, way, implement, tag, mapping, itemid, ...   \n",
       "4                              [enum, django, model]   \n",
       "\n",
       "                                     body-tokens-wov  \\\n",
       "0                        [change, pro, con, article]   \n",
       "1  [collection, base, class, type, type, add, xml...   \n",
       "2  [project, database, bunch, stuff, web, differe...   \n",
       "3  [heard, way, mapping, itemid, sense, scale, nu...   \n",
       "4                              [enum, django, model]   \n",
       "\n",
       "                              title-tokens  \n",
       "0   [sql, server, search, ft, versus, net]  \n",
       "1           [serialization, inherit, type]  \n",
       "2                 [myisam, versus, innodb]  \n",
       "3  [recommend, database, design, tag, tag]  \n",
       "4             [mysql, enum, django, model]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = \"stackOverFlow.csv\"\n",
    "path = PureWindowsPath(r'C:\\Users\\benjamin.bouchard\\Documents\\PERSONNEL\\OCR\\courseNLP\\P5')\n",
    "data = pd.read_csv(f'{path}/{result}')\n",
    "deserialize_list(data,['body-tokens','body-tokens-wov','title-tokens','ntags'])\n",
    "one_doc = 1/len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(df,x_cols,y_col,stratified_col,test_size=0.2):\n",
    "    index = None\n",
    "    try:\n",
    "        index = df.loc[df.duplicated(subset=stratified_col, keep=False),:].index\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if index is not None:\n",
    "        #stratified part\n",
    "        d = df.loc[index,:]\n",
    "        X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(d[x_cols],d[y_col],test_size=test_size,random_state=42,stratify=d[stratified_col])\n",
    "        # not stratified part\n",
    "        index = df.index.difference(index)\n",
    "        d = df.loc[index,:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(d[x_cols],d[y_col],test_size=test_size,random_state=42)\n",
    "\n",
    "        return (\n",
    "                pd.concat([X_train_s,X_train]),\n",
    "                pd.concat([X_test_s,X_test]),\n",
    "                pd.concat([y_train_s,y_train]),\n",
    "                pd.concat([y_test_s,y_test])\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(df[x_cols],df[y_col],test_size=test_size,random_state=42)\n",
    "\n",
    "        return (\n",
    "            X_train,\n",
    "            X_test,\n",
    "            y_train,\n",
    "            y_test\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hit rate\n",
    "def positive_rate(y_true,y_predicted):\n",
    "    results = list(\n",
    "                accumulate(\n",
    "                    iterable=range(y_true.shape[1]), \n",
    "                    func=lambda acc,i: acc + confusion_matrix(y_true[:,i],y_predicted[:,i]),\n",
    "                    initial=np.zeros((2,2))\n",
    "                    )\n",
    "                )[-1]\n",
    "    return np.trace(results)/np.sum(results)\n",
    "    \n",
    "# Jaccard score\n",
    "jscore = lambda y_true, y_pred: jaccard_score(y_true, y_pred, average='samples',zero_division=0.0)\n",
    "\n",
    "# coverage rate\n",
    "def coverage_rate(ys_true,ys_predicted):\n",
    " cov = lambda y_true,y_predicted: len(set(y_true).intersection(set(y_predicted)))/(len(y_true) + len(ys_predicted)) if (len(y_true) + len(ys_predicted)) > 0 else 0.0\n",
    " return np.mean(list(map(lambda t: cov(t[0],t[1]),zip(ys_true,ys_predicted))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_name = lambda name: 'filtered-' + name\n",
    "\n",
    "filter_tokens = lambda df,Y_tokens,nb_tags: df[Y_tokens].apply(make_filter_list(include=n_most_used_tokens(nb_tags,df,Y_tokens)['tokens'].values))\n",
    "\n",
    "save_result = lambda v,n: joblib.dump(value=v,filename=f\"./results/{n}.joblib\")\n",
    "\n",
    "def save_model(results,cs,name):\n",
    "    return joblib.dump(\n",
    "        value={\n",
    "            k:{ c: results[k][c] for c in cs} for k in results.keys() \n",
    "        },\n",
    "        filename=f\"./models/{name}.joblib\"\n",
    "    )\n",
    "\n",
    "load_results = lambda i: joblib.load(filename=f\"./results/supvs_res{i}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_unsupervised_classify(df,body_tokens,title_tokens,Y_tokens,body_max_df,body_min_df,nb_tags,title_min_df,title_max_df,n_term,num_topics,**args):\n",
    "  \n",
    "  #on ne garde que les nb_tags tags les plus utilisés\n",
    "  df[filtered_name(Y_tokens)] = filter_tokens(df,Y_tokens,nb_tags) \n",
    "\n",
    "  #création de la colonne pour stratification (d'après les nb_strat tags les plus courants)\n",
    "  nb_strat = 30\n",
    "  df[filtered_name(Y_tokens) + '-str'] = pipe(df,[\n",
    "    make_filter_list(include=n_most_used_tokens(nb_strat,df,Y_tokens)['tokens'].values),\n",
    "    list_2_str],\n",
    "    filtered_name(Y_tokens)\n",
    "  )\n",
    "  input_tokens = 'tokens'\n",
    "  # filtrage\n",
    "  df[filtered_name(body_tokens)]  = filter_term_docfreq(df,body_tokens,max=body_max_df,min=body_min_df)\n",
    "  df[filtered_name(title_tokens)] = filter_term_docfreq(df,title_tokens,max=title_max_df,min=title_min_df)\n",
    "  \n",
    "  #fusion body|title\n",
    "  df[input_tokens] = merge_tokens(df,[filtered_name(body_tokens),filtered_name(title_tokens)])  \n",
    "  \n",
    "  #split train-test\n",
    "  X_train, X_test, y_train, y_test = split_df(df,x_cols=[input_tokens,'Id'],y_col=[Y_tokens,filtered_name(Y_tokens),'Id'],stratified_col =(filtered_name(Y_tokens) + '-str'))\n",
    "  \n",
    "  # fit\n",
    "  (lda,dictionary,corpus) = ldam.fit(X_train[input_tokens],num_topics=num_topics,**args)\n",
    "  \n",
    "  #predict\n",
    "  predicted_tag_probas = ldam.predict(X_test[input_tokens],lda,dictionary,n_term=n_term)\n",
    "  \n",
    "  #score\n",
    "  scores = {}\n",
    "  predicted_tags = predicted_tag_probas[:,:,0]\n",
    "  mlb = MultiLabelBinarizer()#sparse_output=True\n",
    "  # fit encoder with orignal tags plus the ones found by unsupervised process\n",
    "  all_tags = df[filtered_name(Y_tokens)].values.tolist()\n",
    "  all_tags.extend(predicted_tags.tolist())\n",
    "  mlb.fit(all_tags)\n",
    "  t = mlb.transform(y_test[filtered_name(Y_tokens)])\n",
    "  t_predicted = mlb.transform(predicted_tags)\n",
    "  scores['jaccard']  = jscore(csr_matrix(t),csr_matrix(t_predicted))\n",
    "  #scores['positive_rate'] = positive_rate(t,t_predicted)\n",
    "  #scores['coverage_rate'] = coverage_rate(y_test[Y_tokens].values,predicted_tags)\n",
    "\n",
    "  #clean-up\n",
    "  df.drop(columns=[input_tokens,filtered_name(body_tokens),filtered_name(title_tokens),filtered_name(Y_tokens),filtered_name(Y_tokens)+'-str'],inplace=True)\n",
    "  \n",
    "  return {\n",
    "    **{\n",
    "    'model':lda,\n",
    "    'dict':dictionary,\n",
    "    'corpus':corpus,\n",
    "    'predicted_tag_probas':predicted_tag_probas,\n",
    "    't':y_test[Y_tokens].values,\n",
    "    },\n",
    "    **scores\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lda_results(lda_results):\n",
    "    _= list(map(\n",
    "            lambda t: print(\n",
    "                        f\">> {t[0]} << {' '.join(map(lambda x: f'{x}: {t[1][x]}',['jaccard','coverage_rate'])) }\"\n",
    "                        ), \n",
    "            lda_results.items()\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour 5000 samples\n",
    "lda_args = {\n",
    "     'chunksize'  : 2000,\n",
    "     'passes'     : 20,\n",
    "     'iterations' : 400,\n",
    "     'eval_every' : None\n",
    "}\n",
    "lda_params0 ={\n",
    "     'body_tokens':'body-tokens',\n",
    "     'title_tokens':'title-tokens',\n",
    "     'body_max_df':1492*one_doc,\n",
    "     'body_min_df':164*one_doc,\n",
    "     'title_max_df':1.0,\n",
    "     'title_min_df':58*one_doc,     \n",
    "     'n_term':5,\n",
    "     'num_topics':10,\n",
    "     'Y_tokens':'ntags',\n",
    "     'nb_tags':50\n",
    "}\n",
    "lda_params1 ={\n",
    "     'body_tokens':'body-tokens-wov',\n",
    "     'title_tokens':'title-tokens',\n",
    "     'body_max_df':1.0,\n",
    "     'body_min_df':105*one_doc,\n",
    "     'title_max_df':1.0,\n",
    "     'title_min_df':58*one_doc,     \n",
    "     'n_term':5,\n",
    "     'num_topics':10,\n",
    "     'Y_tokens':'ntags',\n",
    "     'nb_tags':50\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_results = {\n",
    "     0: { **do_unsupervised_classify(data,**lda_params0,**lda_args), **{'name':'params0'} },\n",
    "     1: { **do_unsupervised_classify(data,**lda_params1,**lda_args), **{'name':'params1'} }\n",
    "}\n",
    "print_lda_results(lda_results)\n",
    "save_result(lda_results,'lda')\n",
    "save_model(lda_results,['model','dict'],'lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_params = lambda res,k: [ res[k][n] for n in ['model','corpus','dict'] ]\n",
    "vis0 = gensimvis.prepare(*vis_params(lda_results,0), mds=\"mmds\", R=10)\n",
    "vis0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis1 = gensimvis.prepare(*vis_params(lda_results,1), mds=\"mmds\", R=10)\n",
    "vis1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCAFeaturesSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAFeaturesSelection(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer for manual selection of features using sklearn style transform method.  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,level=0.8):\n",
    "        self.__level__ = level\n",
    "        self.__pipe__  = Pipeline([(\"1\",StandardScaler()),(\"2\",PCA())])\n",
    "        self.__range__ = None\n",
    "        pass\n",
    "\n",
    "    def set_params(self,level):\n",
    "        self.__level__ = level\n",
    "\n",
    "    def __range_components__(self):\n",
    "        total = 0\n",
    "        def below(x):\n",
    "            nonlocal total \n",
    "            total = total+x\n",
    "            return (total < self.__level__)\n",
    "        self.__range__ = range(len(list(itertools.takewhile(below, self.__pipe__[-1].explained_variance_ratio_) )))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.__pipe__.fit(X)\n",
    "        self.__range_components__()\n",
    "        return self\n",
    " \n",
    "    def transform(self, X,y=None):\n",
    "        X_trans = self.__pipe__.transform(X)\n",
    "        return X_trans[:,self.__range__]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipe Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "take = lambda n,res: list(map(lambda t: t[n],res))\n",
    "\n",
    "class pipe_params:\n",
    "    \"\"\"helping class to generate grid parameters for a pipe\"\"\"\n",
    "\n",
    "    def __init__(self,pipe):\n",
    "        self.__params__ = pipe.get_params()\n",
    "\n",
    "    @staticmethod\n",
    "    def __is_pipe_params__(pipe):\n",
    "            return lambda s: any(map(lambda x: f\"{x}__\" in s , take(0,pipe.get_params()['steps'])))\n",
    "\n",
    "    @staticmethod\n",
    "    def pipe_params(pipe):\n",
    "        p = pipe_params.__is_pipe_params__(pipe)\n",
    "        return {k:v for (k,v) in pipe.get_params().items() if p(k) }\n",
    "\n",
    "    @staticmethod\n",
    "    def __overload_pipe_params__(pipe_params,param_grid):\n",
    "        return {**pipe_params,**param_grid}\n",
    "\n",
    "    @staticmethod\n",
    "    def gen_grid_params(param_grid):\n",
    "        \"\"\"\n",
    "        >param_grid={'a':[1,2],'b':[3,4],'c':[7]}\n",
    "        >gen_grid_params(param_grid)\n",
    "        gives:\n",
    "        [   {'a': 1, 'b': 3, 'c': 7},\n",
    "            {'a': 1, 'b': 4, 'c': 7},\n",
    "            {'a': 2, 'b': 3, 'c': 7},\n",
    "            {'a': 2, 'b': 4, 'c': 7}\n",
    "        ]\n",
    "        \"\"\"\n",
    "        dict_keys     = lambda d : [k for k in d.keys()]\n",
    "        dict_prod_len = lambda d : math.prod(map(lambda x: len(x),d.values()))\n",
    "        return [ {k:v for (k,v) in t}\n",
    "                for t in [\n",
    "                        list(zip(x[0],x[1])) for x in \n",
    "                            list( zip(\n",
    "                                    [dict_keys(param_grid)]*dict_prod_len(param_grid),\n",
    "                                    list(itertools.product(*param_grid.values())),\n",
    "                                )\n",
    "                            )\n",
    "                        ]\n",
    "        ]\n",
    "\n",
    "    def make(self,param_grid):\n",
    "        \"\"\"Generate grid parameters for pipe based on param_grid\"\"\"\n",
    "        return [\n",
    "            pipe_params.__overload_pipe_params__(self.__params__,pg)\n",
    "            for pg in pipe_params.gen_grid_params(param_grid)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### supervised_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_classify(data,pipe,pipe_params,\n",
    "                        vect=None,nb_tags= None,body_tokens=None,body_max_df=None,body_min_df=None,title_tokens=None,title_max_df=None,title_min_df=None,Y_tokens=None,\n",
    "                         X_=None,Z_=None,y_train_v_=None,y_test_v_=None,all_tokens_=None,\n",
    "                        col_id='Id'):\n",
    "    if vect is not None:\n",
    "        input_tokens = 'input-tokens'\n",
    "        stratified_col_ = 'filtered-tag-str'\n",
    "\n",
    "        data[filtered_name(Y_tokens)] = filter_tokens(data,Y_tokens,nb_tags)\n",
    "        data[stratified_col_] = data[filtered_name(Y_tokens)].apply(list_2_str)\n",
    "\n",
    "        data[filtered_name(body_tokens)]  = filter_term_docfreq(data,body_tokens,max=body_max_df,min=body_min_df)\n",
    "        data[filtered_name(title_tokens)] = filter_term_docfreq(data,title_tokens,max=title_max_df,min=title_min_df)\n",
    "    \n",
    "        data[input_tokens] = merge_tokens(data,[filtered_name(title_tokens),filtered_name(body_tokens)])  \n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = split_df(data,\n",
    "                                                    x_cols=[input_tokens,col_id],\n",
    "                                                    y_col=[Y_tokens,filtered_name(Y_tokens),col_id],\n",
    "                                                    stratified_col = stratified_col_\n",
    "                                                )\n",
    "\n",
    "        y_train_v = y_train[filtered_name(Y_tokens)].values\n",
    "        y_test_v  = y_test[filtered_name(Y_tokens)].values   \n",
    "        all_tokens = data[filtered_name(Y_tokens)].values\n",
    "        #X\n",
    "        (X,vectorizer) = vectz.vectorize(X_train[input_tokens],vect)\n",
    "        Z              = vectorizer.transform(X_test[input_tokens])\n",
    "    else:\n",
    "        vectorizer = None\n",
    "        X = X_\n",
    "        Z = Z_\n",
    "        y_train_v = y_train_v_\n",
    "        y_test_v = y_test_v_\n",
    "        all_tokens = all_tokens_\n",
    "\n",
    "    #y: encode tags\n",
    "    mlb = MultiLabelBinarizer()#sparse_output=True\n",
    "    mlb.fit(all_tokens)\n",
    "    y   = mlb.transform(y_train_v)\n",
    "    t   = mlb.transform(y_test_v)\n",
    "    \n",
    "    #fit\n",
    "    pipe.set_params(**pipe_params)\n",
    "    model = OneVsRestClassifier(pipe).fit(X,y)\n",
    "    \n",
    "    #predict\n",
    "    t_predicted = model.predict(Z)\n",
    "\n",
    "    #scores\n",
    "    # for coverage rate we used actual tags.\n",
    "    scores = {}\n",
    "    # jaccard score requires csr format\n",
    "    scores['jaccard']       = jscore(csr_matrix(t),csr_matrix(t_predicted))\n",
    "    #scores['positive_rate'] = positive_rate(t,t_predicted)\n",
    "    #scores['coverage_rate'] = coverage_rate(y_test[Y_tokens],mlb.inverse_transform(t_predicted))\n",
    "\n",
    "    # return {\n",
    "    #     **{'X':X,'y':y,'Z':Z,'t':t,'t_predicted':t_predicted,\n",
    "    #     'train_ids':X_train[col_id].values,'test_ids':X_test[col_id].values,\n",
    "    #     'mlb':mlb,'vectorizer':vectorizer,'model':model},\n",
    "    #     **scores\n",
    "    # }\n",
    "    return {\n",
    "        **{'X':X,'y':y,'Z':Z,'t':t,'t_predicted':t_predicted,\n",
    "    #    'train_ids':X_train[col_id].values,'test_ids':X_test[col_id].values,\n",
    "        'mlb':mlb,'vectorizer':vectorizer,'model':model},\n",
    "        **scores\n",
    "    }\n",
    "\n",
    "## Run config\n",
    "def do_supervised_classify(data,config):\n",
    "  pipe = config['pipe']\n",
    "  ret = {}\n",
    "  i = 0\n",
    "  for (pipe_params_,params) in list(\n",
    "                                  itertools.product(\n",
    "                                                    pipe_params(pipe).make(config['pipe_params']),\n",
    "                                                    pipe_params.gen_grid_params(config['params'])\n",
    "                                                  )\n",
    "                                  ):\n",
    "    name = config_name(pipe_params_,params)\n",
    "    print(f\"Run config:\\n{name} ...\")\n",
    "    ret[i] = { \n",
    "                  **supervised_classify(data,**params,pipe=pipe,pipe_params=pipe_params_), \n",
    "                  **{'pipe_params':pipe_params_,'params':params,'name':name}\n",
    "                }\n",
    "    i = i + 1\n",
    "  return ret\n",
    "\n",
    "\n",
    "def run_config(data,config,num):\n",
    "    results = do_supervised_classify(data,config)\n",
    "    save_result(results,f'supvs_res{num}')\n",
    "    save_model(results,['model','vectorizer','name'],f'supvs_res{num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooks for config names transforms\n",
    "vect_reg_exp = re.compile(\"<class 'sklearn.feature_extraction.text.(.*)'>\")\n",
    "vect_trans = lambda x: vect_reg_exp.findall(f\"{x}\")[0] if len(vect_reg_exp.findall(f\"{x}\")) > 0 else x\n",
    "#  log_loss  Vs squared_error\n",
    "loss_trans = lambda x : 'Logistic' if x == 'squared_error' else 'SVM'\n",
    "df_trans = lambda x : f\"{round(100*x,3)}%\"\n",
    "\n",
    "transformers = {'classifier__loss': loss_trans, 'vect': vect_trans,'body_min_df':df_trans,'body_max_df':df_trans,'title_min_df':df_trans,'title_max_df':df_trans}\n",
    "# tranform config names\n",
    "transform = lambda k,v: transformers[k](v) if transformers.get(k) is not None else v\n",
    "# define names of config\n",
    "dict_keys = lambda d0,d1: [k for k in {**d0,**d1}.keys()]\n",
    "fmt_str  = lambda l: ('|'.join(list(map(lambda t: f\"{t[0]}: {{{t[1]}}}\",l))))\n",
    "\n",
    "\n",
    "config_name = lambda d0,d1: fmt_str(\n",
    "      filter(\n",
    "        lambda t: t[1] in  dict_keys(d0,d1),\n",
    "        [\n",
    "          ('Classifier','classifier__loss'),('Vect','vect'), ('var-level','var-reducer__level'), ('nb_tags','nb_tags'),\n",
    "          ('body_min_df','body_min_df'),('body_max_df','body_max_df'), ('title_min_df','title_min_df'),\n",
    "          ('title_max_df','title_max_df')\n",
    "        ],\n",
    "      )\n",
    "    ).format_map({k: transform(k,v) for (k,v) in {**d0,**d1}.items()})\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_supervised_results(results):\n",
    "    print(\">> ===== results ===== <<\")\n",
    "    for k in range(len(results)):\n",
    "        #k = list(results.keys())[i]\n",
    "        name = results[k]['name']\n",
    "        print(f\">> {k} <<{name}:\")\n",
    "        print(' | '.join(map(lambda r: f\"{r}: {results[k][r]}\",['jaccard'])) + \"\\n\")  #,'coverage_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_param      = lambda d,n: d['params'][n] if d['params'].get(n) is not None else 'N.A'\n",
    "get_pipe_param = lambda d,n: d['pipe_params'][n] if d['pipe_params'].get(n) is not None else 'N.A'\n",
    "\n",
    "get_row_result = lambda d: [loss_trans(get_pipe_param(d,'classifier__loss'))] + \\\n",
    "                            [vect_trans(get_param(d,'vect'))] + \\\n",
    "                            [get_param(d,n) for n in ['nb_tags','body_tokens']] + \\\n",
    "                            [ str(round(100*get_param(d,n),4)) + '%' if get_param(d,n) != 'N.A' else get_param(d,n)  for n in ['body_min_df','body_max_df','title_min_df','title_max_df'] ] + \\\n",
    "                            [d['jaccard']] + \\\n",
    "                            [len(d['pipe_params']['steps']) == 2]   \n",
    "\n",
    "get_results = lambda res: [get_row_result(res[k]) for k in res.keys()]\n",
    "\n",
    "#\n",
    "def set_vect(d,v):\n",
    "     for k in d.keys():\n",
    "          d[k]['params']['vect'] = v\n",
    "\n",
    "df_results = lambda res: pd.DataFrame(data=get_results(res),columns=['Classifier','Vectoriser','Nb Tags','Body tokens','body_min_df','body_max_df','title_min_df','title_max_df','score','scaler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bow Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config0 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[164*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                     ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[482*one_doc], \n",
    "        'title_min_df':[206*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = do_supervised_classify(data,config2)\n",
    "# print_supervised_results(results2)\n",
    "# save_result(results2,'supvs_res2')\n",
    "# save_model(results2,['model','vectorizer','name'],'supvs_res2')\n",
    "run_config(data,config2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[2000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results3 = do_supervised_classify(data,config3)\n",
    "# print_supervised_results(results3)\n",
    "# save_result(results3,'supvs_res3')\n",
    "# save_model(results3,['model','vectorizer','name'],'supvs_res3')\n",
    "run_config(data,config3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### load pre-train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as dwl\n",
    "dwl.info()['models'].keys() \n",
    "dwl.info()['models']['glove-wiki-gigaword-300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dwl.load('glove-wiki-gigaword-300')\n",
    "w2v = vectz.W2V_Vectoriser(model,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[w2v]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config5 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[482*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[w2v]\n",
    "    }\n",
    "}\n",
    "run_config(data,config5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT & USE Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content for BERT and USE embedding vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['contents'] = data['Body'].apply(do_parse,args=(MyHTMLParser(filter= lambda x: True if x == 'code' else False ,container=Strcontainer() ),))\n",
    "data['contents'] = pipe(data,\n",
    "[\n",
    "make_remove_signs_str(signs=[\"\\n\"]),\n",
    "make_remove_signs_str(signs=['\"\\\"'])\n",
    "],\n",
    "'contents')\n",
    "# Merge content and title\n",
    "data['input-content'] = data.apply(lambda r: (r['Title'] + '. ' ) + r['contents'] ,axis=1)\n",
    "data['input-content'] =  pipe(data,[lower_str],'input-content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT et USE vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_vect(data,input_tokens):\n",
    "    stratified_col_ = 'filtered-tag-str'\n",
    "    nb_tags = 100\n",
    "    Y_tokens = 'ntags'\n",
    "    data[filtered_name(Y_tokens)] = filter_tokens(data,Y_tokens,nb_tags)\n",
    "    data[stratified_col_] = data[filtered_name(Y_tokens)].apply(list_2_str)\n",
    "    col_id='Id'\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_df(data,\n",
    "                                                x_cols=[input_tokens,col_id],\n",
    "                                                y_col=[Y_tokens,filtered_name(Y_tokens),col_id],\n",
    "                                                stratified_col = stratified_col_\n",
    "                                                )\n",
    "\n",
    "    features = {}\n",
    "    for mode in ['HF','TFhub','USE']:\n",
    "        \n",
    "        if mode != 'USE':\n",
    "            vect_obj = vectz.BERT_Vectoriser(mode=mode)\n",
    "        else:\n",
    "            vect_obj = vectz.USE_Vectorizer()\n",
    "\n",
    "        #vectorize\n",
    "        tmp = vect_obj.fit_transform(data[input_tokens])    \n",
    "        features[mode] = { \n",
    "            'X_':tmp[X_train.index],\n",
    "            'Z_':tmp[X_test.index],\n",
    "            'y_train_v_':y_train[filtered_name(Y_tokens)].values,\n",
    "            'y_test_v_':y_test[filtered_name(Y_tokens)].values,\n",
    "            'all_tokens_': data[filtered_name(Y_tokens)].values\n",
    "        }\n",
    "\n",
    "    #save\n",
    "    joblib.dump(\n",
    "    value=features,\n",
    "    filename=f\"./tmp/berts_use_params.joblib\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#berts_use_params = do_vect(data,'input-content')\n",
    "berts_use_params =  joblib.load(filename=f\"./tmp/berts_use_params.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = berts_use_params['HF']['X_']\n",
    "# Z_ = berts_use_params['HF']['Z_']\n",
    "# y_train_v_ = berts_use_params['HF']['y_train_v_']\n",
    "# y_test_v_  =  berts_use_params['HF']['y_test_v_']\n",
    "# all_tokens_ =  berts_use_params['HF']['all_tokens_']\n",
    "\n",
    "config7 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]}, #'squared_error'\n",
    "    'params':{\n",
    "        # 'X_': [X_[0:1000]],\n",
    "        # 'Z_':[Z_[0:200]],\n",
    "        # 'y_train_v_': [y_train_v_[0:1000]],\n",
    "        # 'y_test_v_':[y_test_v_[0:200]],\n",
    "        # 'all_tokens_':[all_tokens_]\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['HF'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config([],config7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config8 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]},\n",
    "    'params':{\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['TFhub'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results8 = do_supervised_classify(data,config8)\n",
    "# save_result(results8,'supvs_res8')\n",
    "# save_model(results8,['model','vectorizer','name'],'supvs_res8')\n",
    "# pd.DataFrame(df_results(results8))\n",
    "run_config([],config8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config9 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]},\n",
    "    'params':{\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['USE'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config([],config9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "[res0,res1,res2,res3,res4,res5,res7,res8,res9] = list(map(lambda i: load_results(i),[0,1,2,3,4,5,7,8,9]))\n",
    "set_vect(res4,'w2v')\n",
    "set_vect(res5,'w2v')\n",
    "set_vect(res7,'BERT_HF')\n",
    "set_vect(res8,'BERT_TFhub')\n",
    "set_vect(res9,'USE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Vectoriser</th>\n",
       "      <th>Nb Tags</th>\n",
       "      <th>Body tokens</th>\n",
       "      <th>body_min_df</th>\n",
       "      <th>body_max_df</th>\n",
       "      <th>title_min_df</th>\n",
       "      <th>title_max_df</th>\n",
       "      <th>score</th>\n",
       "      <th>scaler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.018150</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.127420</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.122054</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.029113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.017248</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.031620</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.097383</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.077323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.046688</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.028892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.044701</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.129706</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.122976</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.072046</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.273206</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.226119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.467294</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.316204</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.377323</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.398761</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>BERT_HF</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.564910</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>BERT_HF</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.564880</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>BERT_TFhub</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>BERT_TFhub</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.552005</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>USE</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.535896</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>USE</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.590596</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier       Vectoriser Nb Tags      Body tokens body_min_df  \\\n",
       "0   Logistic  CountVectorizer      50      body-tokens     0.5999%   \n",
       "1   Logistic  TfidfVectorizer      50      body-tokens     0.5999%   \n",
       "2   Logistic  CountVectorizer     100      body-tokens     0.5999%   \n",
       "3   Logistic  TfidfVectorizer     100      body-tokens     0.5999%   \n",
       "4        SVM  CountVectorizer      50      body-tokens     0.5999%   \n",
       "5        SVM  TfidfVectorizer      50      body-tokens     0.5999%   \n",
       "6        SVM  CountVectorizer     100      body-tokens     0.5999%   \n",
       "7        SVM  TfidfVectorizer     100      body-tokens     0.5999%   \n",
       "0   Logistic  CountVectorizer      50      body-tokens     1.7631%   \n",
       "1   Logistic  TfidfVectorizer      50      body-tokens     1.7631%   \n",
       "2   Logistic  CountVectorizer     100      body-tokens     1.7631%   \n",
       "3   Logistic  TfidfVectorizer     100      body-tokens     1.7631%   \n",
       "4        SVM  CountVectorizer      50      body-tokens     1.7631%   \n",
       "5        SVM  TfidfVectorizer      50      body-tokens     1.7631%   \n",
       "6        SVM  CountVectorizer     100      body-tokens     1.7631%   \n",
       "7        SVM  TfidfVectorizer     100      body-tokens     1.7631%   \n",
       "0   Logistic  CountVectorizer      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "2   Logistic  CountVectorizer     100  body-tokens-wov     0.3841%   \n",
       "3   Logistic  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "4        SVM  CountVectorizer      50  body-tokens-wov     0.3841%   \n",
       "5        SVM  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "6        SVM  CountVectorizer     100  body-tokens-wov     0.3841%   \n",
       "7        SVM  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "2        SVM  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "3        SVM  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic              w2v      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic              w2v     100  body-tokens-wov     0.3841%   \n",
       "2        SVM              w2v      50  body-tokens-wov     0.3841%   \n",
       "3        SVM              w2v     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic              w2v      50      body-tokens     1.7631%   \n",
       "1   Logistic              w2v     100      body-tokens     1.7631%   \n",
       "2        SVM              w2v      50      body-tokens     1.7631%   \n",
       "3        SVM              w2v     100      body-tokens     1.7631%   \n",
       "0   Logistic          BERT_HF     N.A              N.A         N.A   \n",
       "1        SVM          BERT_HF     N.A              N.A         N.A   \n",
       "0   Logistic       BERT_TFhub     N.A              N.A         N.A   \n",
       "1        SVM       BERT_TFhub     N.A              N.A         N.A   \n",
       "0   Logistic              USE     N.A              N.A         N.A   \n",
       "1        SVM              USE     N.A              N.A         N.A   \n",
       "\n",
       "  body_max_df title_min_df title_max_df     score  scaler  \n",
       "0     5.4576%      0.2122%       100.0%  0.029724    True  \n",
       "1     5.4576%      0.2122%       100.0%  0.048099    True  \n",
       "2     5.4576%      0.2122%       100.0%  0.018150    True  \n",
       "3     5.4576%      0.2122%       100.0%  0.036366    True  \n",
       "4     5.4576%      0.2122%       100.0%  0.127420    True  \n",
       "5     5.4576%      0.2122%       100.0%  0.122054    True  \n",
       "6     5.4576%      0.2122%       100.0%  0.064221    True  \n",
       "7     5.4576%      0.2122%       100.0%  0.059194    True  \n",
       "0     5.4576%      0.7535%       100.0%  0.029113    True  \n",
       "1     5.4576%      0.7535%       100.0%  0.046729    True  \n",
       "2     5.4576%      0.7535%       100.0%  0.017248    True  \n",
       "3     5.4576%      0.7535%       100.0%  0.031620    True  \n",
       "4     5.4576%      0.7535%       100.0%  0.097383    True  \n",
       "5     5.4576%      0.7535%       100.0%  0.077323    True  \n",
       "6     5.4576%      0.7535%       100.0%  0.046688    True  \n",
       "7     5.4576%      0.7535%       100.0%  0.039876    True  \n",
       "0      100.0%      0.2122%       100.0%  0.028892    True  \n",
       "1      100.0%      0.2122%       100.0%  0.044701    True  \n",
       "2      100.0%      0.2122%       100.0%  0.018158    True  \n",
       "3      100.0%      0.2122%       100.0%  0.057034    True  \n",
       "4      100.0%      0.2122%       100.0%  0.129706    True  \n",
       "5      100.0%      0.2122%       100.0%  0.122976    True  \n",
       "6      100.0%      0.2122%       100.0%  0.072046    True  \n",
       "7      100.0%      0.2122%       100.0%  0.062126    True  \n",
       "0      100.0%      0.2122%       100.0%  0.027321   False  \n",
       "1      100.0%      0.2122%       100.0%  0.022612   False  \n",
       "2      100.0%      0.2122%       100.0%  0.301102   False  \n",
       "3      100.0%      0.2122%       100.0%  0.283077   False  \n",
       "0      100.0%      0.2122%       100.0%  0.273206   False  \n",
       "1      100.0%      0.2122%       100.0%  0.226119   False  \n",
       "2      100.0%      0.2122%       100.0%  0.301102   False  \n",
       "3      100.0%      0.2122%       100.0%  0.283077   False  \n",
       "0     5.4576%      0.7535%       100.0%  0.467294   False  \n",
       "1     5.4576%      0.7535%       100.0%  0.316204   False  \n",
       "2     5.4576%      0.7535%       100.0%  0.377323   False  \n",
       "3     5.4576%      0.7535%       100.0%  0.398761   False  \n",
       "0         N.A          N.A          N.A  0.564910   False  \n",
       "1         N.A          N.A          N.A  0.564880   False  \n",
       "0         N.A          N.A          N.A  0.559082   False  \n",
       "1         N.A          N.A          N.A  0.552005   False  \n",
       "0         N.A          N.A          N.A  0.535896   False  \n",
       "1         N.A          N.A          N.A  0.590596   False  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat( [ df_results(res0),df_results(res1),df_results(res2),df_results(res3),df_results(res4),df_results(res5),df_results(res7),df_results(res8),df_results(res9)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b8c1420fd4668c3125a30c19ada9a198156baf5bd8e9521b9be91b166ac9fd7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Modules import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from html.parser import HTMLParser\n",
    "from itertools import accumulate,chain,takewhile\n",
    "import itertools\n",
    "import warnings\n",
    "import itertools\n",
    "from pathlib import Path,PureWindowsPath,PurePosixPath\n",
    "\n",
    "#wordcloud\n",
    "import wordcloud\n",
    "\n",
    "# \n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#pygments\n",
    "from pygments.lexers import guess_lexer\n",
    "from pygments.util  import ClassNotFound\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import jaccard_score,confusion_matrix\n",
    "\n",
    "\n",
    "#gensim\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "\n",
    "#bitarray\n",
    "# from  bitarray import bitarray\n",
    "# from  bitarray.util import ba2int\n",
    "\n",
    "#joblib\n",
    "import joblib\n",
    "\n",
    "#local\n",
    "from Bouchard_Benjamin_1_notebook_exploration_0520222 import *\n",
    "import vectorizer as vectz\n",
    "import lda as ldam\n",
    "\n",
    "# from importlib import reload\n",
    "# reload(vectz)\n",
    "warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src  import classify\n",
    "from src import io_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Id</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>FavoriteCount</th>\n",
       "      <th>AnswerCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SQL Server 2008 Full Text Search (FTS) versus ...</td>\n",
       "      <td>&lt;p&gt;I know there have been questions in the pas...</td>\n",
       "      <td>&lt;sql-server&gt;&lt;sql-server-2008&gt;&lt;full-text-search...</td>\n",
       "      <td>499247</td>\n",
       "      <td>40</td>\n",
       "      <td>18582</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XML Serialization and Inherited Types</td>\n",
       "      <td>&lt;p&gt;Following on from my &lt;a href=\"https://stack...</td>\n",
       "      <td>&lt;c#&gt;&lt;xml&gt;&lt;inheritance&gt;&lt;serialization&gt;&lt;xml-seri...</td>\n",
       "      <td>20084</td>\n",
       "      <td>86</td>\n",
       "      <td>56816</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MyISAM versus InnoDB</td>\n",
       "      <td>&lt;p&gt;I'm working on a projects which involves a ...</td>\n",
       "      <td>&lt;mysql&gt;&lt;database&gt;&lt;performance&gt;&lt;innodb&gt;&lt;myisam&gt;</td>\n",
       "      <td>20148</td>\n",
       "      <td>887</td>\n",
       "      <td>301985</td>\n",
       "      <td>390</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recommended SQL database design for tags or ta...</td>\n",
       "      <td>&lt;p&gt;I've heard of a few ways to implement taggi...</td>\n",
       "      <td>&lt;sql&gt;&lt;database-design&gt;&lt;tags&gt;&lt;data-modeling&gt;&lt;ta...</td>\n",
       "      <td>20856</td>\n",
       "      <td>325</td>\n",
       "      <td>118552</td>\n",
       "      <td>307</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specifying a mySQL ENUM in a Django model</td>\n",
       "      <td>&lt;p&gt;How do I go about specifying and using an E...</td>\n",
       "      <td>&lt;python&gt;&lt;mysql&gt;&lt;django&gt;&lt;django-models&gt;&lt;enums&gt;</td>\n",
       "      <td>21454</td>\n",
       "      <td>99</td>\n",
       "      <td>61572</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  SQL Server 2008 Full Text Search (FTS) versus ...   \n",
       "1              XML Serialization and Inherited Types   \n",
       "2                               MyISAM versus InnoDB   \n",
       "3  Recommended SQL database design for tags or ta...   \n",
       "4          Specifying a mySQL ENUM in a Django model   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <p>I know there have been questions in the pas...   \n",
       "1  <p>Following on from my <a href=\"https://stack...   \n",
       "2  <p>I'm working on a projects which involves a ...   \n",
       "3  <p>I've heard of a few ways to implement taggi...   \n",
       "4  <p>How do I go about specifying and using an E...   \n",
       "\n",
       "                                                Tags      Id  Score  \\\n",
       "0  <sql-server><sql-server-2008><full-text-search...  499247     40   \n",
       "1  <c#><xml><inheritance><serialization><xml-seri...   20084     86   \n",
       "2     <mysql><database><performance><innodb><myisam>   20148    887   \n",
       "3  <sql><database-design><tags><data-modeling><ta...   20856    325   \n",
       "4      <python><mysql><django><django-models><enums>   21454     99   \n",
       "\n",
       "   ViewCount  FavoriteCount  AnswerCount  \n",
       "0      18582             26            5  \n",
       "1      56816             42            7  \n",
       "2     301985            390           25  \n",
       "3     118552            307            6  \n",
       "4      61572             21            9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = io_file.load_input()\n",
    "one_doc = 1/len(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour 5000 samples\n",
    "lda_args = {\n",
    "     'chunksize'  : 2000,\n",
    "     'passes'     : 20,\n",
    "     'iterations' : 400,\n",
    "     'eval_every' : None\n",
    "}\n",
    "lda_params0 ={\n",
    "     'body_tokens':'body-tokens',\n",
    "     'title_tokens':'title-tokens',\n",
    "     'body_max_df':1492*one_doc,\n",
    "     'body_min_df':164*one_doc,\n",
    "     'title_max_df':1.0,\n",
    "     'title_min_df':58*one_doc,     \n",
    "     'n_term':5,\n",
    "     'num_topics':10,\n",
    "     'Y_tokens':'ntags',\n",
    "     'nb_tags':50\n",
    "}\n",
    "lda_params1 ={\n",
    "     'body_tokens':'body-tokens-wov',\n",
    "     'title_tokens':'title-tokens',\n",
    "     'body_max_df':1.0,\n",
    "     'body_min_df':105*one_doc,\n",
    "     'title_max_df':1.0,\n",
    "     'title_min_df':58*one_doc,     \n",
    "     'n_term':5,\n",
    "     'num_topics':10,\n",
    "     'Y_tokens':'ntags',\n",
    "     'nb_tags':50\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lda_results(lda_results):\n",
    "    _= list(map(\n",
    "            lambda t: print(\n",
    "                        f\">> {t[0]} << {' '.join(map(lambda x: f'{x}: {t[1][x]}',['jaccard','coverage_rate'])) }\"\n",
    "                        ), \n",
    "            lda_results.items()\n",
    "        )\n",
    "    )\n",
    "\n",
    "lda_results = {\n",
    "     0: { **classify.unsupervised_classify(data,**lda_params0,**lda_args), **{'name':'params0'} },\n",
    "     1: { **classify.unsupervised_classify(data,**lda_params1,**lda_args), **{'name':'params1'} }\n",
    "}\n",
    "\n",
    "print_lda_results(lda_results)\n",
    "io_file.save_result(lda_results,'lda')\n",
    "io_file.save_model(lda_results,['model','dict'],'lda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis_params = lambda res,k: [ res[k][n] for n in ['model','corpus','dict'] ]\n",
    "vis0 = gensimvis.prepare(*vis_params(lda_results,0), mds=\"mmds\", R=10)\n",
    "vis0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis1 = gensimvis.prepare(*vis_params(lda_results,1), mds=\"mmds\", R=10)\n",
    "vis1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### supervised_classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Run config\n",
    "def do_supervised_classify(data,config):\n",
    "  pipe = config['pipe']\n",
    "  ret = {}\n",
    "  i = 0\n",
    "  for (pipe_params_,params) in list(\n",
    "                                  itertools.product(\n",
    "                                                    pipe_params(pipe).make(config['pipe_params']),\n",
    "                                                    pipe_params.gen_grid_params(config['params'])\n",
    "                                                  )\n",
    "                                  ):\n",
    "    name = config_name(pipe_params_,params)\n",
    "    print(f\"Run config:\\n{name} ...\")\n",
    "    ret[i] = { \n",
    "                  **supervised_classify(data,**params,pipe=pipe,pipe_params=pipe_params_), \n",
    "                  **{'pipe_params':pipe_params_,'params':params,'name':name}\n",
    "                }\n",
    "    i = i + 1\n",
    "  return ret\n",
    "\n",
    "\n",
    "def run_config(data,config,num):\n",
    "    results = do_supervised_classify(data,config)\n",
    "    save_result(results,f'supvs_res{num}')\n",
    "    save_model(results,['model','vectorizer','name'],f'supvs_res{num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooks for config names transforms\n",
    "vect_reg_exp = re.compile(\"<class 'sklearn.feature_extraction.text.(.*)'>\")\n",
    "vect_trans = lambda x: vect_reg_exp.findall(f\"{x}\")[0] if len(vect_reg_exp.findall(f\"{x}\")) > 0 else x\n",
    "#  log_loss  Vs squared_error\n",
    "loss_trans = lambda x : 'Logistic' if x == 'squared_error' else 'SVM'\n",
    "df_trans = lambda x : f\"{round(100*x,3)}%\"\n",
    "\n",
    "transformers = {'classifier__loss': loss_trans, 'vect': vect_trans,'body_min_df':df_trans,'body_max_df':df_trans,'title_min_df':df_trans,'title_max_df':df_trans}\n",
    "# tranform config names\n",
    "transform = lambda k,v: transformers[k](v) if transformers.get(k) is not None else v\n",
    "# define names of config\n",
    "dict_keys = lambda d0,d1: [k for k in {**d0,**d1}.keys()]\n",
    "fmt_str  = lambda l: ('|'.join(list(map(lambda t: f\"{t[0]}: {{{t[1]}}}\",l))))\n",
    "\n",
    "\n",
    "config_name = lambda d0,d1: fmt_str(\n",
    "      filter(\n",
    "        lambda t: t[1] in  dict_keys(d0,d1),\n",
    "        [\n",
    "          ('Classifier','classifier__loss'),('Vect','vect'), ('var-level','var-reducer__level'), ('nb_tags','nb_tags'),\n",
    "          ('body_min_df','body_min_df'),('body_max_df','body_max_df'), ('title_min_df','title_min_df'),\n",
    "          ('title_max_df','title_max_df')\n",
    "        ],\n",
    "      )\n",
    "    ).format_map({k: transform(k,v) for (k,v) in {**d0,**d1}.items()})\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_supervised_results(results):\n",
    "    print(\">> ===== results ===== <<\")\n",
    "    for k in range(len(results)):\n",
    "        #k = list(results.keys())[i]\n",
    "        name = results[k]['name']\n",
    "        print(f\">> {k} <<{name}:\")\n",
    "        print(' | '.join(map(lambda r: f\"{r}: {results[k][r]}\",['jaccard'])) + \"\\n\")  #,'coverage_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_param      = lambda d,n: d['params'][n] if d['params'].get(n) is not None else 'N.A'\n",
    "get_pipe_param = lambda d,n: d['pipe_params'][n] if d['pipe_params'].get(n) is not None else 'N.A'\n",
    "\n",
    "get_row_result = lambda d: [loss_trans(get_pipe_param(d,'classifier__loss'))] + \\\n",
    "                            [vect_trans(get_param(d,'vect'))] + \\\n",
    "                            [get_param(d,n) for n in ['nb_tags','body_tokens']] + \\\n",
    "                            [ str(round(100*get_param(d,n),4)) + '%' if get_param(d,n) != 'N.A' else get_param(d,n)  for n in ['body_min_df','body_max_df','title_min_df','title_max_df'] ] + \\\n",
    "                            [d['jaccard']] + \\\n",
    "                            [len(d['pipe_params']['steps']) == 2]   \n",
    "\n",
    "get_results = lambda res: [get_row_result(res[k]) for k in res.keys()]\n",
    "\n",
    "#\n",
    "def set_vect(d,v):\n",
    "     for k in d.keys():\n",
    "          d[k]['params']['vect'] = v\n",
    "\n",
    "df_results = lambda res: pd.DataFrame(data=get_results(res),columns=['Classifier','Vectoriser','Nb Tags','Body tokens','body_min_df','body_max_df','title_min_df','title_max_df','score','scaler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bow Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config0 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[164*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                     ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[482*one_doc], \n",
    "        'title_min_df':[206*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[CountVectorizer,TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = do_supervised_classify(data,config2)\n",
    "# print_supervised_results(results2)\n",
    "# save_result(results2,'supvs_res2')\n",
    "# save_model(results2,['model','vectorizer','name'],'supvs_res2')\n",
    "run_config(data,config2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config3 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[2000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[TfidfVectorizer]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results3 = do_supervised_classify(data,config3)\n",
    "# print_supervised_results(results3)\n",
    "# save_result(results3,'supvs_res3')\n",
    "# save_model(results3,['model','vectorizer','name'],'supvs_res3')\n",
    "run_config(data,config3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### load pre-train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as dwl\n",
    "dwl.info()['models'].keys() \n",
    "dwl.info()['models']['glove-wiki-gigaword-300']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dwl.load('glove-wiki-gigaword-300')\n",
    "w2v = vectz.W2V_Vectoriser(model,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config4 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens-wov'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[105*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1.0], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[w2v]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config(data,config4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config5 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(fit_intercept=False,n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['squared_error','hinge'],'classifier__max_iter':[5000]},\n",
    "    'params':{\n",
    "        'body_tokens':['body-tokens'],\n",
    "        'title_tokens':['title-tokens'], \n",
    "        'Y_tokens':['ntags'],\n",
    "        'nb_tags':[50,100], \n",
    "        'body_min_df':[482*one_doc], \n",
    "        'title_min_df':[58*one_doc], \n",
    "        'body_max_df':[1492*one_doc], \n",
    "        'title_max_df':[1.0],\n",
    "        'vect':[w2v]\n",
    "    }\n",
    "}\n",
    "run_config(data,config5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT & USE Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content for BERT and USE embedding vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['contents'] = data['Body'].apply(do_parse,args=(MyHTMLParser(filter= lambda x: True if x == 'code' else False ,container=Strcontainer() ),))\n",
    "data['contents'] = pipe(data,\n",
    "[\n",
    "make_remove_signs_str(signs=[\"\\n\"]),\n",
    "make_remove_signs_str(signs=['\"\\\"'])\n",
    "],\n",
    "'contents')\n",
    "# Merge content and title\n",
    "data['input-content'] = data.apply(lambda r: (r['Title'] + '. ' ) + r['contents'] ,axis=1)\n",
    "data['input-content'] =  pipe(data,[lower_str],'input-content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT et USE vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_vect(data,input_tokens):\n",
    "    stratified_col_ = 'filtered-tag-str'\n",
    "    nb_tags = 100\n",
    "    Y_tokens = 'ntags'\n",
    "    data[filtered_name(Y_tokens)] = filter_tokens(data,Y_tokens,nb_tags)\n",
    "    data[stratified_col_] = data[filtered_name(Y_tokens)].apply(list_2_str)\n",
    "    col_id='Id'\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = split_df(data,\n",
    "                                                x_cols=[input_tokens,col_id],\n",
    "                                                y_col=[Y_tokens,filtered_name(Y_tokens),col_id],\n",
    "                                                stratified_col = stratified_col_\n",
    "                                                )\n",
    "\n",
    "    features = {}\n",
    "    for mode in ['HF','TFhub','USE']:\n",
    "        \n",
    "        if mode != 'USE':\n",
    "            vect_obj = vectz.BERT_Vectoriser(mode=mode)\n",
    "        else:\n",
    "            vect_obj = vectz.USE_Vectorizer()\n",
    "\n",
    "        #vectorize\n",
    "        tmp = vect_obj.fit_transform(data[input_tokens])    \n",
    "        features[mode] = { \n",
    "            'X_':tmp[X_train.index],\n",
    "            'Z_':tmp[X_test.index],\n",
    "            'y_train_v_':y_train[filtered_name(Y_tokens)].values,\n",
    "            'y_test_v_':y_test[filtered_name(Y_tokens)].values,\n",
    "            'all_tokens_': data[filtered_name(Y_tokens)].values\n",
    "        }\n",
    "\n",
    "    #save\n",
    "    joblib.dump(\n",
    "    value=features,\n",
    "    filename=f\"./tmp/berts_use_params.joblib\")\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#berts_use_params = do_vect(data,'input-content')\n",
    "berts_use_params =  joblib.load(filename=f\"./tmp/berts_use_params.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_ = berts_use_params['HF']['X_']\n",
    "# Z_ = berts_use_params['HF']['Z_']\n",
    "# y_train_v_ = berts_use_params['HF']['y_train_v_']\n",
    "# y_test_v_  =  berts_use_params['HF']['y_test_v_']\n",
    "# all_tokens_ =  berts_use_params['HF']['all_tokens_']\n",
    "\n",
    "config7 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]}, #'squared_error'\n",
    "    'params':{\n",
    "        # 'X_': [X_[0:1000]],\n",
    "        # 'Z_':[Z_[0:200]],\n",
    "        # 'y_train_v_': [y_train_v_[0:1000]],\n",
    "        # 'y_test_v_':[y_test_v_[0:200]],\n",
    "        # 'all_tokens_':[all_tokens_]\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['HF'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config([],config7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "config8 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]},\n",
    "    'params':{\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['TFhub'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results8 = do_supervised_classify(data,config8)\n",
    "# save_result(results8,'supvs_res8')\n",
    "# save_model(results8,['model','vectorizer','name'],'supvs_res8')\n",
    "# pd.DataFrame(df_results(results8))\n",
    "run_config([],config8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### config9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "config9 = {\n",
    "    'pipe':Pipeline( \n",
    "                steps=[\n",
    "                    ('classifier' , SGDClassifier(n_jobs=-1,random_state=42))\n",
    "                ]\n",
    "            ),\n",
    "    'pipe_params':{'classifier__loss':['hinge'],'classifier__max_iter':[7500]},\n",
    "    'params':{\n",
    "        **{ k:[v]  for (k,v) in berts_use_params['USE'].items()}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config([],config9,9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage rÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "[res0,res1,res2,res3,res4,res5,res7,res8,res9] = list(map(lambda i: load_results(i),[0,1,2,3,4,5,7,8,9]))\n",
    "set_vect(res4,'w2v')\n",
    "set_vect(res5,'w2v')\n",
    "set_vect(res7,'BERT_HF')\n",
    "set_vect(res8,'BERT_TFhub')\n",
    "set_vect(res9,'USE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Vectoriser</th>\n",
       "      <th>Nb Tags</th>\n",
       "      <th>Body tokens</th>\n",
       "      <th>body_min_df</th>\n",
       "      <th>body_max_df</th>\n",
       "      <th>title_min_df</th>\n",
       "      <th>title_max_df</th>\n",
       "      <th>score</th>\n",
       "      <th>scaler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.048099</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.018150</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.036366</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.127420</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.122054</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.064221</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>0.5999%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.059194</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.029113</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.017248</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.031620</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.097383</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.077323</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.046688</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.028892</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.044701</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.018158</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.129706</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.122976</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.072046</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.062126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.027321</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.273206</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.226119</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.301102</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens-wov</td>\n",
       "      <td>0.3841%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.2122%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.283077</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.467294</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.316204</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>50</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.377323</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>w2v</td>\n",
       "      <td>100</td>\n",
       "      <td>body-tokens</td>\n",
       "      <td>1.7631%</td>\n",
       "      <td>5.4576%</td>\n",
       "      <td>0.7535%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.398761</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>BERT_HF</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.564910</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>BERT_HF</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.564880</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>BERT_TFhub</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>BERT_TFhub</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.552005</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>USE</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.535896</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>USE</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>N.A</td>\n",
       "      <td>0.590596</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classifier       Vectoriser Nb Tags      Body tokens body_min_df  \\\n",
       "0   Logistic  CountVectorizer      50      body-tokens     0.5999%   \n",
       "1   Logistic  TfidfVectorizer      50      body-tokens     0.5999%   \n",
       "2   Logistic  CountVectorizer     100      body-tokens     0.5999%   \n",
       "3   Logistic  TfidfVectorizer     100      body-tokens     0.5999%   \n",
       "4        SVM  CountVectorizer      50      body-tokens     0.5999%   \n",
       "5        SVM  TfidfVectorizer      50      body-tokens     0.5999%   \n",
       "6        SVM  CountVectorizer     100      body-tokens     0.5999%   \n",
       "7        SVM  TfidfVectorizer     100      body-tokens     0.5999%   \n",
       "0   Logistic  CountVectorizer      50      body-tokens     1.7631%   \n",
       "1   Logistic  TfidfVectorizer      50      body-tokens     1.7631%   \n",
       "2   Logistic  CountVectorizer     100      body-tokens     1.7631%   \n",
       "3   Logistic  TfidfVectorizer     100      body-tokens     1.7631%   \n",
       "4        SVM  CountVectorizer      50      body-tokens     1.7631%   \n",
       "5        SVM  TfidfVectorizer      50      body-tokens     1.7631%   \n",
       "6        SVM  CountVectorizer     100      body-tokens     1.7631%   \n",
       "7        SVM  TfidfVectorizer     100      body-tokens     1.7631%   \n",
       "0   Logistic  CountVectorizer      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "2   Logistic  CountVectorizer     100  body-tokens-wov     0.3841%   \n",
       "3   Logistic  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "4        SVM  CountVectorizer      50  body-tokens-wov     0.3841%   \n",
       "5        SVM  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "6        SVM  CountVectorizer     100  body-tokens-wov     0.3841%   \n",
       "7        SVM  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "2        SVM  TfidfVectorizer      50  body-tokens-wov     0.3841%   \n",
       "3        SVM  TfidfVectorizer     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic              w2v      50  body-tokens-wov     0.3841%   \n",
       "1   Logistic              w2v     100  body-tokens-wov     0.3841%   \n",
       "2        SVM              w2v      50  body-tokens-wov     0.3841%   \n",
       "3        SVM              w2v     100  body-tokens-wov     0.3841%   \n",
       "0   Logistic              w2v      50      body-tokens     1.7631%   \n",
       "1   Logistic              w2v     100      body-tokens     1.7631%   \n",
       "2        SVM              w2v      50      body-tokens     1.7631%   \n",
       "3        SVM              w2v     100      body-tokens     1.7631%   \n",
       "0   Logistic          BERT_HF     N.A              N.A         N.A   \n",
       "1        SVM          BERT_HF     N.A              N.A         N.A   \n",
       "0   Logistic       BERT_TFhub     N.A              N.A         N.A   \n",
       "1        SVM       BERT_TFhub     N.A              N.A         N.A   \n",
       "0   Logistic              USE     N.A              N.A         N.A   \n",
       "1        SVM              USE     N.A              N.A         N.A   \n",
       "\n",
       "  body_max_df title_min_df title_max_df     score  scaler  \n",
       "0     5.4576%      0.2122%       100.0%  0.029724    True  \n",
       "1     5.4576%      0.2122%       100.0%  0.048099    True  \n",
       "2     5.4576%      0.2122%       100.0%  0.018150    True  \n",
       "3     5.4576%      0.2122%       100.0%  0.036366    True  \n",
       "4     5.4576%      0.2122%       100.0%  0.127420    True  \n",
       "5     5.4576%      0.2122%       100.0%  0.122054    True  \n",
       "6     5.4576%      0.2122%       100.0%  0.064221    True  \n",
       "7     5.4576%      0.2122%       100.0%  0.059194    True  \n",
       "0     5.4576%      0.7535%       100.0%  0.029113    True  \n",
       "1     5.4576%      0.7535%       100.0%  0.046729    True  \n",
       "2     5.4576%      0.7535%       100.0%  0.017248    True  \n",
       "3     5.4576%      0.7535%       100.0%  0.031620    True  \n",
       "4     5.4576%      0.7535%       100.0%  0.097383    True  \n",
       "5     5.4576%      0.7535%       100.0%  0.077323    True  \n",
       "6     5.4576%      0.7535%       100.0%  0.046688    True  \n",
       "7     5.4576%      0.7535%       100.0%  0.039876    True  \n",
       "0      100.0%      0.2122%       100.0%  0.028892    True  \n",
       "1      100.0%      0.2122%       100.0%  0.044701    True  \n",
       "2      100.0%      0.2122%       100.0%  0.018158    True  \n",
       "3      100.0%      0.2122%       100.0%  0.057034    True  \n",
       "4      100.0%      0.2122%       100.0%  0.129706    True  \n",
       "5      100.0%      0.2122%       100.0%  0.122976    True  \n",
       "6      100.0%      0.2122%       100.0%  0.072046    True  \n",
       "7      100.0%      0.2122%       100.0%  0.062126    True  \n",
       "0      100.0%      0.2122%       100.0%  0.027321   False  \n",
       "1      100.0%      0.2122%       100.0%  0.022612   False  \n",
       "2      100.0%      0.2122%       100.0%  0.301102   False  \n",
       "3      100.0%      0.2122%       100.0%  0.283077   False  \n",
       "0      100.0%      0.2122%       100.0%  0.273206   False  \n",
       "1      100.0%      0.2122%       100.0%  0.226119   False  \n",
       "2      100.0%      0.2122%       100.0%  0.301102   False  \n",
       "3      100.0%      0.2122%       100.0%  0.283077   False  \n",
       "0     5.4576%      0.7535%       100.0%  0.467294   False  \n",
       "1     5.4576%      0.7535%       100.0%  0.316204   False  \n",
       "2     5.4576%      0.7535%       100.0%  0.377323   False  \n",
       "3     5.4576%      0.7535%       100.0%  0.398761   False  \n",
       "0         N.A          N.A          N.A  0.564910   False  \n",
       "1         N.A          N.A          N.A  0.564880   False  \n",
       "0         N.A          N.A          N.A  0.559082   False  \n",
       "1         N.A          N.A          N.A  0.552005   False  \n",
       "0         N.A          N.A          N.A  0.535896   False  \n",
       "1         N.A          N.A          N.A  0.590596   False  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat( [ df_results(res0),df_results(res1),df_results(res2),df_results(res3),df_results(res4),df_results(res5),df_results(res7),df_results(res8),df_results(res9)])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b8c1420fd4668c3125a30c19ada9a198156baf5bd8e9521b9be91b166ac9fd7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
